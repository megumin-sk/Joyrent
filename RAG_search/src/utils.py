from typing import List, Dict, Tuple
from langchain_text_splitters import RecursiveCharacterTextSplitter
import logging

logger = logging.getLogger(__name__)


def smart_split(text: str, max_length: int = 500, overlap: int = 50, domain: str = "game") -> List[str]:
    """
    ä½¿ç”¨ LangChain çš„ RecursiveCharacterTextSplitter è¿›è¡Œæ™ºèƒ½åˆ‡åˆ†
    å®ƒä¼šé€’å½’åœ°å°è¯•æŒ‰æ®µè½ã€å¥å­ã€å•è¯ç­‰å±‚çº§è¿›è¡Œåˆ‡åˆ†ï¼Œå°½å¯èƒ½ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ã€‚
    
    Args:
        text: è¦åˆ‡åˆ†çš„æ–‡æœ¬
        max_length: å•ä¸ª chunk çš„æœ€å¤§å­—ç¬¦æ•°
        overlap: chunk ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼ˆä¿è¯ä¸Šä¸‹æ–‡è¿è´¯æ€§ï¼‰
        domain: é¢†åŸŸç±»å‹ ('game', 'general')
    
    Returns:
        åˆ‡åˆ†åçš„æ–‡æœ¬å—åˆ—è¡¨
    """
    if not text or len(text.strip()) == 0:
        return []
    
    # æŒ‰é¢†åŸŸè®¾ç½®åˆ†éš”ç¬¦ï¼ˆæ¸¸æˆè¯„è®ºæœ‰ã€æ ‡ç­¾ã€‘æ ¼å¼ï¼‰
    separators = {
        "game": ["\nã€", "\n\n", "\n", "ã€‚\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""],
        "general": ["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""],
    }
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=max_length,
        chunk_overlap=overlap,
        separators=separators.get(domain, separators["general"]),
        add_start_index=True,  # è®°å½•æ¯ä¸ª chunk åœ¨åŸæ–‡çš„ä½ç½®
    )
    
    chunks = text_splitter.split_text(text)
    return chunks if chunks else []


def smart_split_with_metrics(
    text: str, 
    max_length: int = 500, 
    overlap: int = 50,
    domain: str = "game"
) -> Tuple[List[str], Dict]:
    """
    è¿”å› (chunks, metrics)ï¼Œä¾¿äºç›‘æ§å’Œè°ƒè¯•
    
    Returns:
        - chunks: åˆ‡åˆ†åçš„æ–‡æœ¬å—
        - metrics: åˆ‡åˆ†è´¨é‡æŒ‡æ ‡
    """
    if not text or len(text.strip()) == 0:
        return [], {}
    
    separators = {
        "game": ["\nã€", "\n\n", "\n", "ã€‚\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""],
        "general": ["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""],
    }
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=max_length,
        chunk_overlap=overlap,
        separators=separators.get(domain, separators["general"]),
        add_start_index=True,
    )
    
    chunks = text_splitter.split_text(text)
    
    # ğŸ“Š è´¨é‡æŒ‡æ ‡
    metrics = {
        "total_chunks": len(chunks),
        "avg_chunk_size": sum(len(c) for c in chunks) / len(chunks) if chunks else 0,
        "min_chunk_size": min((len(c) for c in chunks), default=0),
        "max_chunk_size": max((len(c) for c in chunks), default=0),
        "coverage_ratio": sum(len(c) for c in chunks) / len(text) if len(text) > 0 else 0,
    }
    
    logger.info(f"âœ… Split metrics: {metrics}")
    
    return chunks, metrics


def load_and_split(file_path: str, max_length: int = 500, overlap: int = 50) -> Tuple[List[str], Dict]:
    """
    åŠ è½½æ–‡æ¡£ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰å¹¶åˆ‡åˆ†
    
    æ”¯æŒæ ¼å¼: PDF, DOCX, TXT, MARKDOWN, HTML, EXCEL
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        max_length: chunk å¤§å°
        overlap: chunk é‡å 
    
    Returns:
        - chunks: åˆ‡åˆ†åçš„æ–‡æœ¬
        - metadata: æ–‡ä»¶å…ƒæ•°æ® + åˆ‡åˆ†æŒ‡æ ‡
    """
    from document_loader import UniversalDocumentLoader
    from pathlib import Path
    
    # Step 1: åŠ è½½æ–‡æ¡£
    loader = UniversalDocumentLoader()
    texts, metadata = loader.load_document(file_path)
    
    # Step 2: åˆå¹¶æ‰€æœ‰æ–‡æœ¬ï¼ˆå› ä¸ºæŸäº›æ ¼å¼å¯èƒ½åˆ†å¤šéƒ¨åˆ†ï¼‰
    full_text = "\n\n".join(texts)
    
    # Step 3: åˆ‡åˆ†
    chunks, split_metrics = smart_split_with_metrics(full_text, max_length, overlap)
    
    # Step 4: åˆå¹¶å…ƒæ•°æ®
    combined_metadata = {
        **metadata,
        **split_metrics,
    }
    
    logger.info(f"ğŸ“„ Successfully processed: {Path(file_path).name}")
    logger.info(f"   Chunks: {split_metrics['total_chunks']}, Avg size: {split_metrics['avg_chunk_size']:.0f}")
    
    return chunks, combined_metadata
