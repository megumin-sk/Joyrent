import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.optim import AdamW
from transformers import BertTokenizer, get_linear_schedule_with_warmup
from tqdm import tqdm
import os

# å¯¼å…¥æˆ‘ä»¬å†™çš„æ¨¡å—
from config import Config
from dataset import RentalDataset
from model import MultiHeadBERT
from utils import EarlyStopping

# ================= æŸå¤±å‡½æ•° =================
def loss_fn(outputs, targets):
    class_weights = torch.tensor([6.0, 4.0, 4.0, 1.0]).to(outputs[0].device)
    loss_fct = nn.CrossEntropyLoss(weight=class_weights) 
    # ====================================================
    
    total_loss = 0
    for i, output in enumerate(outputs):
        loss = loss_fct(output, targets[:, i])
        total_loss += loss
        
    return total_loss

# ================= è®­ç»ƒå¾ªç¯ =================
def train_fn(data_loader, model, optimizer, device, scheduler):
    model.train()
    final_loss = 0
    
    loop = tqdm(data_loader, total=len(data_loader), leave=True)
    for data in loop:
        ids = data['ids'].to(device)
        mask = data['mask'].to(device)
        targets = data['targets'].to(device)

        optimizer.zero_grad()
        outputs = model(ids, mask)
        loss = loss_fn(outputs, targets)
        
        # 1. åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼šå¦‚æœæ¢¯åº¦èŒƒæ•°è¶…è¿‡ 1.0ï¼Œå°±æŠŠå®ƒå¼ºè¡Œåˆ‡æ–­ã€‚
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # 2. æ›´æ–°å‚æ•°
        optimizer.step()
        scheduler.step()

        final_loss += loss.item()
        loop.set_description(f"Train Loss: {loss.item():.4f}")

    return final_loss / len(data_loader)

# ================= éªŒè¯å¾ªç¯ =================
def eval_fn(data_loader, model, device):
    model.eval()
    final_loss = 0
    with torch.no_grad():
        for data in tqdm(data_loader, total=len(data_loader), desc="Validating"):
            ids = data['ids'].to(device)
            mask = data['mask'].to(device)
            targets = data['targets'].to(device)

            outputs = model(ids, mask)
            loss = loss_fn(outputs, targets)
            final_loss += loss.item()
            
    return final_loss / len(data_loader)

# ================= ä¸»å…¥å£ =================
if __name__ == "__main__":
    print(f"ğŸš€ Training Config: Device={Config.DEVICE}, Batch={Config.TRAIN_BATCH_SIZE}")
    
    # 1. å‡†å¤‡ Tokenizer
    tokenizer = BertTokenizer.from_pretrained(Config.BERT_PATH)
    
    # 2. å‡†å¤‡ Dataset å’Œ DataLoader
    train_dataset = RentalDataset(Config.TRAIN_FILE, tokenizer, Config.MAX_LEN, Config.TARGET_COLS)
    val_dataset = RentalDataset(Config.VAL_FILE, tokenizer, Config.MAX_LEN, Config.TARGET_COLS)
    
    train_loader = DataLoader(train_dataset, batch_size=Config.TRAIN_BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=Config.VALID_BATCH_SIZE, shuffle=False)
    
    # 3. åˆå§‹åŒ–æ¨¡å‹
    model = MultiHeadBERT(Config)
    model.to(Config.DEVICE)
    
    # 4. ä¼˜åŒ–å™¨ä¸è°ƒåº¦å™¨
    optimizer = AdamW(model.parameters(), lr=Config.LEARNING_RATE)
    total_steps = len(train_loader) * Config.EPOCHS
    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=0, num_training_steps=total_steps
    )
    
    # 5. æ—©åœæœºåˆ¶ (æ¨¡å‹ä¿å­˜è·¯å¾„)
    save_path = os.path.join(Config.MODEL_SAVE_DIR, "best_model.bin")
    early_stopping = EarlyStopping(patience=3, path=save_path)
    
    # 6. å¼€å§‹ Epoch å¾ªç¯
    for epoch in range(Config.EPOCHS):
        print(f"\n======== Epoch {epoch + 1}/{Config.EPOCHS} ========")
        
        train_loss = train_fn(train_loader, model, optimizer, Config.DEVICE, scheduler)
        val_loss = eval_fn(val_loader, model, Config.DEVICE)
        
        print(f"ğŸ“ˆ Avg Train Loss: {train_loss:.4f}")
        print(f"ğŸ“‰ Avg Valid Loss: {val_loss:.4f}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ—©åœæˆ–ä¿å­˜æ¨¡å‹
        early_stopping(val_loss, model)
        if early_stopping.early_stop:
            print("âš ï¸ Early stopping triggered!")
            break

    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³æ¨¡å‹å·²ä¿å­˜ã€‚")