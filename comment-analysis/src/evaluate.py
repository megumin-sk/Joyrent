import torch
import numpy as np
import os
import sys
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score, classification_report
from torch.utils.data import DataLoader
from transformers import BertTokenizer

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from config import Config
from model import MultiHeadBERT
from dataset import RentalDataset

# æ ‡ç­¾æ˜ å°„ (ç”¨äºæ‰“å°æŠ¥å‘Š)
LABEL_NAMES = ["Negative (å·®)", "Neutral (ä¸­)", "Positive (å¥½)", "None (æœªæåŠ)"]

def evaluate():
    print(f"ğŸš€ å¼€å§‹è¯„ä¼°æ¨¡å‹...")
    print(f"   è®¾å¤‡: {Config.DEVICE}")
    
    # 1. ç¡®å®šæµ‹è¯•é›†è·¯å¾„
    # å¦‚æœ Config é‡Œæ²¡å®šä¹‰ TEST_FILEï¼Œå°±æ‰‹åŠ¨æ‹¼ä¸€ä¸ª
    test_file = getattr(Config, 'TEST_FILE', os.path.join(Config.DATA_DIR, 'test.json'))
    
    if not os.path.exists(test_file):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æµ‹è¯•é›†æ–‡ä»¶ -> {test_file}")
        return

    # 2. åŠ è½½æ•°æ®
    print(f"   åŠ è½½æµ‹è¯•é›†: {test_file}")
    tokenizer = BertTokenizer.from_pretrained(Config.BERT_PATH)
    test_dataset = RentalDataset(test_file, tokenizer, Config.MAX_LEN, Config.TARGET_COLS)
    # batch_size å¯ä»¥è®¾å¤§ç‚¹ï¼Œå› ä¸ºä¸éœ€è¦åå‘ä¼ æ’­ï¼Œçœæ˜¾å­˜
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

    # 3. åŠ è½½æ¨¡å‹
    model = MultiHeadBERT(Config)
    model_path = os.path.join(Config.MODEL_SAVE_DIR, 'best_model.bin')
    
    if not os.path.exists(model_path):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ -> {model_path}")
        print("   è¯·å…ˆè¿è¡Œ train.py è¿›è¡Œè®­ç»ƒï¼")
        return
        
    model.load_state_dict(torch.load(model_path, map_location=Config.DEVICE))
    model.to(Config.DEVICE)
    model.eval() # å¼€å¯è¯„ä¼°æ¨¡å¼

    # 4. æ”¶é›†é¢„æµ‹ç»“æœ
    # å­˜å‚¨ 8 ä¸ªç»´åº¦çš„çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾
    # ç»“æ„: [ [dim0_preds...], [dim1_preds...], ... ]
    all_targets = [[] for _ in range(len(Config.TARGET_COLS))]
    all_preds = [[] for _ in range(len(Config.TARGET_COLS))]

    print("   æ­£åœ¨è¿›è¡Œæ¨ç†...")
    with torch.no_grad():
        for data in tqdm(test_loader):
            ids = data['ids'].to(Config.DEVICE)
            mask = data['mask'].to(Config.DEVICE)
            targets = data['targets'].to(Config.DEVICE) # [Batch, 8]

            outputs = model(ids, mask) # List of 8 tensors

            for i, logits in enumerate(outputs):
                # è·å–é¢„æµ‹ç±»åˆ« (Argmax)
                preds = torch.argmax(logits, dim=1)
                
                # æ”¶é›†ç»“æœ (è½¬å› CPU å­˜å…¥åˆ—è¡¨)
                all_targets[i].extend(targets[:, i].cpu().numpy())
                all_preds[i].extend(preds.cpu().numpy())

    # 5. è®¡ç®—å¹¶æ‰“å°æŒ‡æ ‡
    print("\n" + "="*60)
    print(f"{'ç»´åº¦ (Dimension)':<15} | {'Accuracy':<10} | {'Macro F1':<10}")
    print("-" * 60)
    
    avg_acc = 0
    avg_f1 = 0
    
    # è¯¦ç»†æŠ¥å‘Šå­˜å‚¨
    details = []

    for i, col in enumerate(Config.TARGET_COLS):
        y_true = all_targets[i]
        y_pred = all_preds[i]
        
        # è®¡ç®—åŸºç¡€æŒ‡æ ‡
        acc = accuracy_score(y_true, y_pred)
        # Macro F1: å¯¹æ‰€æœ‰ç±»åˆ«(0,1,2,3)ä¸€è§†åŒä»æ±‚å¹³å‡ï¼Œèƒ½åæ˜ æ¨¡å‹åœ¨å°æ ·æœ¬ç±»åˆ«ä¸Šçš„è¡¨ç°
        f1 = f1_score(y_true, y_pred, average='macro')
        
        avg_acc += acc
        avg_f1 += f1
        
        print(f"{col:<15} | {acc:.4f}     | {f1:.4f}")
        
        # ç”Ÿæˆè¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        report = classification_report(
            y_true, 
            y_pred, 
            labels=[0, 1, 2, 3], 
            target_names=LABEL_NAMES,
            zero_division=0 # é˜²æ­¢é™¤é›¶è­¦å‘Š
        )
        details.append((col, report))

    print("-" * 60)
    print(f"{'OVERALL (å¹³å‡)':<15} | {avg_acc/8:.4f}     | {avg_f1/8:.4f}")
    print("="*60)

    # 6. æ‰“å°è¯¦ç»†æŠ¥å‘Š (å¯é€‰ï¼Œå¦‚æœåªæƒ³çœ‹æ€»è§ˆå¯ä»¥æ³¨é‡Šæ‰)
    print("\nğŸ“ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š (æŒ‰ç»´åº¦):\n")
    for col, report in details:
        print(f"### {col} ###")
        print(report)
        print("-" * 30)

if __name__ == "__main__":
    evaluate()