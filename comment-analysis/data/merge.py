import json
import os
import random

# ================= é…ç½®åŒºåŸŸ =================
# 1. æ­£å¸¸æ•°æ®æº (ä½ çš„ BERT æ•°æ®é›†)
# è¿™äº›è¯„è®ºå°†è¢«æ ‡è®°ä¸º 1 (Normal)
NORMAL_FILES = [
    r'D:\workspace\JoyRent\SwitchRent\comment-analysis\data\processed\train.json',
    r'D:\workspace\JoyRent\SwitchRent\comment-analysis\data\processed\val.json',
    r'D:\workspace\JoyRent\SwitchRent\comment-analysis\data\processed\test.json'
]

# 2. åƒåœ¾æ•°æ®æº (ä½ åˆšæ‰ç”Ÿæˆçš„ 150 æ¡)
# è¿™äº›è¯„è®ºå°†è¢«æ ‡è®°ä¸º 0 (Spam)
# å‡è®¾ä½ æŠŠåˆšæ‰ç”Ÿæˆçš„ 150 æ¡ä¿å­˜åˆ°äº†è¿™ä¸ªè·¯å¾„ï¼Œå¦‚æœæ–‡ä»¶åä¸åŒè¯·ä¿®æ”¹
SPAM_FILE = r'D:\workspace\JoyRent\SwitchRent\comment-analysis\data\raw\spam.json'

# 3. è¾“å‡ºç›®æ ‡è·¯å¾„ (ä½ æŒ‡å®šçš„ä½ç½®)
OUTPUT_FILE = r'D:\workspace\JoyRent\SwitchRent\comment-analysis\data\svm\svmset.json'

# æ ‡ç­¾å®šä¹‰
LABEL_SPAM = 0   # åƒåœ¾
LABEL_NORMAL = 1 # æ­£å¸¸
# ===========================================

def merge_datasets():
    print("ğŸš€ å¼€å§‹åˆå¹¶ SVM è®­ç»ƒæ•°æ®...")
    
    combined_data = []
    
    # --- ç¬¬ä¸€æ­¥ï¼šåŠ è½½æ­£å¸¸æ•°æ® ---
    print(f"\nğŸ“¦ æ­£åœ¨è¯»å–æ­£å¸¸è¯„è®º (Label={LABEL_NORMAL})...")
    normal_count = 0
    for file_path in NORMAL_FILES:
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for item in data:
                    text = item.get('text', '').strip()
                    if text:
                        combined_data.append({
                            "text": text,
                            "label": LABEL_NORMAL
                        })
                        normal_count += 1
            print(f"   - å·²åŠ è½½ {os.path.basename(file_path)}")
        else:
            print(f"   âš ï¸ æ‰¾ä¸åˆ°æ–‡ä»¶: {file_path}")

    # --- ç¬¬äºŒæ­¥ï¼šåŠ è½½åƒåœ¾æ•°æ® ---
    print(f"\nğŸ—‘ï¸ æ­£åœ¨è¯»å–åƒåœ¾è¯„è®º (Label={LABEL_SPAM})...")
    spam_count = 0
    if os.path.exists(SPAM_FILE):
        with open(SPAM_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for item in data:
                text = item.get('text', '').strip()
                if text:
                    combined_data.append({
                        "text": text,
                        "label": LABEL_SPAM
                    })
                    spam_count += 1
        print(f"   - å·²åŠ è½½ {os.path.basename(SPAM_FILE)}")
    else:
        print(f"   âŒ ä¸¥é‡è­¦å‘Šï¼šæ‰¾ä¸åˆ°åƒåœ¾æ•°æ®æ–‡ä»¶ {SPAM_FILE}")
        print("   è¯·ç¡®ä¿ä½ å·²ç»æŠŠé‚£ 150 æ¡æ•°æ®ä¿å­˜åˆ°äº†è¿™ä¸ªä½ç½®ï¼")

    # --- ç¬¬ä¸‰æ­¥ï¼šæ‰“ä¹±ä¸ä¿å­˜ ---
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - æ­£å¸¸æ•°æ®: {normal_count} æ¡")
    print(f"   - åƒåœ¾æ•°æ®: {spam_count} æ¡")
    print(f"   - æ€»è®¡: {len(combined_data)} æ¡")

    print("\nğŸ”€ æ­£åœ¨æ‰“ä¹±æ•°æ®é¡ºåº...")
    random.seed(42) # å›ºå®šç§å­ï¼Œä¿è¯æ¯æ¬¡ç»“æœä¸€è‡´
    random.shuffle(combined_data)

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(OUTPUT_FILE)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"   - åˆ›å»ºç›®å½•: {output_dir}")

    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜è‡³: {OUTPUT_FILE}")
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(combined_data, f, ensure_ascii=False, indent=2)

    print("\nâœ… åˆå¹¶å®Œæˆï¼ç°åœ¨ä½ å¯ä»¥ç”¨è¿™ä¸ªæ–‡ä»¶å»è®­ç»ƒ SVM äº†ã€‚")

if __name__ == "__main__":
    merge_datasets()