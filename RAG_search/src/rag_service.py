import uvicorn
import dashscope
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dashscope import TextEmbedding
from typing import List, Dict, Any, Optional
import sys
import os
import tempfile
import shutil

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.config import Config
from src.database import get_db_connection, init_db, get_games_stock
from src.utils import smart_split, smart_split_with_metrics, load_and_split
from src.model_factory import ModelFactory
import torch
import torch.nn.functional as F

# è®¾ç½® API Key
dashscope.api_key = Config.DASHSCOPE_API_KEY

app = FastAPI(title="RAG Search Service")

# é…ç½® CORSï¼Œå…è®¸å‰ç«¯è®¿é—®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class AddDocRequest(BaseModel):
    game_id: Optional[int]
    category: str
    content: str

class FileUploadResponse(BaseModel):
    success: bool
    message: str
    chunks_count: int
    metadata: Dict[str, Any]

class SearchRequest(BaseModel):
    query: str
    game_id: int = None # å¯é€‰ï¼šåªæœæŸä¸ªæ¸¸æˆ

@app.on_event("startup")
async def startup_event():
    """æœåŠ¡å¯åŠ¨æ—¶åˆå§‹åŒ–æ•°æ®åº“å’Œæ¨¡å‹"""
    init_db()
    
    global bert_model, bert_tokenizer, device
    print(f"Loading BERT Intent Model from {Config.BERT_MODEL_PATH}...")
    try:
        device = torch.device(Config.DEVICE)
        bert_tokenizer = ModelFactory.get_tokenizer(Config.BERT_MODEL_PATH)
        bert_model = ModelFactory.get_model(
             model_path=Config.BERT_MODEL_PATH
        )
        bert_model.to(device)
        bert_model.eval() # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
        print("âœ… BERT Model loaded successfully.")
    except Exception as e:
        import traceback
        print(f"âŒ Failed to load BERT model: {e}")
        print(traceback.format_exc())

@app.post("/rag/upload", response_model=FileUploadResponse)
async def upload_document(
    file: UploadFile = File(...),
    game_id: Optional[int] = None,
    category: str = "document"
):
    """
    ä¸Šä¼ æ–‡æ¡£ï¼ˆæ”¯æŒ PDF, WORD, EXCEL, MARKDOWN, HTML, TXTï¼‰
    è‡ªåŠ¨åŠ è½½å¹¶åˆ‡åˆ†ï¼Œç„¶åå­˜å…¥å‘é‡åº“
    
    Args:
        file: ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆPDF, DOCX, TXT ç­‰ï¼‰
        game_id: å…³è”çš„æ¸¸æˆ IDï¼ˆå¯é€‰ï¼‰
        category: æ–‡æ¡£ç±»åˆ«
    
    Returns:
        ä¸Šä¼ ç»“æœï¼ŒåŒ…å«åˆ‡åˆ†æ•°é‡å’Œå…ƒæ•°æ®
    """
    if not file:
        raise HTTPException(status_code=400, detail="File is required")
    
    # éªŒè¯æ–‡ä»¶ç±»å‹
    allowed_extensions = {'.pdf', '.docx', '.doc', '.txt', '.md', '.xlsx', '.xls', '.html', '.htm'}
    file_ext = os.path.splitext(file.filename)[1].lower()
    
    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400,
            detail=f"Unsupported file format: {file_ext}. Allowed: {allowed_extensions}"
        )
    
    temp_file_path = None
    try:
        # Step 1: ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp:
            temp_file_path = tmp.name
            content = await file.read()
            tmp.write(content)
        
        print(f"ğŸ“„ Processing file: {file.filename} ({len(content)} bytes)")
        
        # Step 2: åŠ è½½å’Œåˆ‡åˆ†æ–‡æ¡£
        chunks, metadata = load_and_split(temp_file_path, max_length=500, overlap=50)
        
        if not chunks:
            raise HTTPException(status_code=400, detail="File content is empty or cannot be processed")
        
        print(f"âœ… Split into {len(chunks)} chunks")
        
        # Step 3: ç”Ÿæˆå‘é‡å¹¶å­˜å…¥æ•°æ®åº“
        embeddings = []
        resp = TextEmbedding.call(
            model=TextEmbedding.Models.text_embedding_v1,
            input=chunks
        )
        
        if resp.status_code == 200:
            raw_embeddings = resp.output['embeddings']
            embeddings = [item['embedding'] for item in raw_embeddings]
        else:
            raise Exception(f"DashScope Embedding Error: {resp.message}")
        
        # Step 4: æ‰¹é‡å­˜å…¥ PostgreSQL
        conn = get_db_connection()
        try:
            with conn.cursor() as cur:
                data_values = [
                    (game_id, category, chunk, vec)
                    for chunk, vec in zip(chunks, embeddings)
                ]
                
                insert_sql = """
                INSERT INTO rag_documents (game_id, category, content, embedding)
                VALUES (%s, %s, %s, %s)
                """
                
                cur.executemany(insert_sql, data_values)
                conn.commit()
                print(f"âœ… Inserted {len(data_values)} vectors into database")
        finally:
            conn.close()
        
        return FileUploadResponse(
            success=True,
            message=f"Successfully processed {file.filename}",
            chunks_count=len(chunks),
            metadata=metadata
        )
        
    except Exception as e:
        import traceback
        print(f"âŒ Error processing file: {e}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"Error processing file: {str(e)}")
    finally:
        # Step 5: æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_file_path and os.path.exists(temp_file_path):
            try:
                os.remove(temp_file_path)
            except:
                pass

@app.post("/rag/add")
async def add_document(request: AddDocRequest):
    """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡åº“"""
    if not request.content:
        raise HTTPException(status_code=400, detail="Content is required")

    try:
        # 1. æ™ºèƒ½åˆ‡ç‰‡
        chunks = smart_split(request.content)
        print(f"Content split into {len(chunks)} chunks.")
        
        # 2. æ‰¹é‡ç”Ÿæˆå‘é‡
        embeddings = []
        
        resp = TextEmbedding.call(
            model=TextEmbedding.Models.text_embedding_v1,
            input=chunks
        )
        
        if resp.status_code == 200:
            raw_embeddings = resp.output['embeddings']
            embeddings = [item['embedding'] for item in raw_embeddings]
        else:
            raise Exception(f"DashScope Embedding Error: {resp.message}")

        # 3. æ‰¹é‡å­˜å…¥ PostgreSQL
        conn = get_db_connection()
        try:
            with conn.cursor() as cur:
                data_values = [
                    (request.game_id, request.category, chunk, vec)
                    for chunk, vec in zip(chunks, embeddings)
                ]
                
                cur.executemany(
                    "INSERT INTO documents (game_id, category, content, embedding) VALUES (%s, %s, %s, %s)",
                    data_values
                )
            
            conn.commit() # å…¨éƒ¨æˆåŠŸæ‰æäº¤
            print(f"Successfully inserted {len(data_values)} records.")
            
        except Exception as db_err:
            conn.rollback() # æœ‰é”™å°±å…¨éƒ¨å›æ»š
            raise db_err
        finally:
            conn.close()

        return {"message": f"Successfully added {len(chunks)} document chunks."}
            
    except Exception as e:
        print(f"Error adding document: {e}")
        raise HTTPException(status_code=500, detail=str(e))

from dashscope import Generation

# å…¨å±€æ¨¡å‹å˜é‡
bert_model = None
bert_tokenizer = None
device = None

def predict_intent(query: str) -> str:
    """
    ä½¿ç”¨ BERT æ¨¡å‹è¯†åˆ«ç”¨æˆ·æ„å›¾
    è¿”å›: 'rule', 'game', æˆ– 'all'
    """
    global bert_model, bert_tokenizer, device
    
    if not bert_model or not bert_tokenizer:
        print("Warning: BERT model not loaded, falling back to 'all'")
        return 'all'
        
    try:
        # 1. é¢„å¤„ç†
        inputs = bert_tokenizer(
            query, 
            return_tensors="pt", 
            truncation=True, 
            max_length=128, 
            padding=True
        ).to(device)
        
        # 2. æ¨ç†
        with torch.no_grad():
            outputs = bert_model(**inputs)
            logits = outputs.logits
        
        # 3. è·å–ç»“æœ
        probs = F.softmax(logits, dim=1)
        # è·å–æœ€å¤§æ¦‚ç‡çš„ç´¢å¼•
        pred_idx = torch.argmax(probs, dim=1).item()
        # è·å–æœ€å¤§æ¦‚ç‡å€¼
        confidence = probs[0][pred_idx].item()
        
        # æ˜ å°„å›æ ‡ç­¾å­—ç¬¦ä¸²
        intent = Config.BERT_LABEL_MAP.get(pred_idx, 'all')
        
        print("\n---------------------------------------------------")
        print(f"ğŸ•µï¸ BERT Intent Recognition Details for: '{query}'")
        for idx, score in enumerate(probs[0]):
            label_name = Config.BERT_LABEL_MAP.get(idx, f"unknown_{idx}")
            bar_len = int(score.item() * 20)
            bar = 'â–ˆ' * bar_len + 'â–‘' * (20 - bar_len)
            print(f"   - {label_name.ljust(6)}: {bar} {score.item():.4f}")
        print(f"ğŸ‘‰ Final Decision: {intent} (Confidence: {confidence:.4f})")
        print("---------------------------------------------------\n")
        
        # å¯é€‰ï¼šå¦‚æœç½®ä¿¡åº¦å¤ªä½ï¼Œå¯ä»¥å¼ºåˆ¶è½¬ä¸º 'all'
        if confidence < 0.6:
            print(f"Confidence too low ({confidence:.4f}), fallback to 'all'")
            return 'all'
            
        return intent

    except Exception as e:
        print(f"Intent recognition error: {e}")
        return 'all'

# --- æ ¸å¿ƒæ£€ç´¢é€»è¾‘æŠ½å– ---
def _retrieve_documents(query: str, game_id: int = None):
    # 1. æ„å›¾è¯†åˆ«
    intent = predict_intent(query)
    print(f"User Query: {query} | Predicted Intent: {intent}")

    # 2. å‘é‡åŒ–
    resp = TextEmbedding.call(
        model=TextEmbedding.Models.text_embedding_v1,
        input=query
    )
    
    if resp.status_code != 200:
        raise Exception(f"Embedding failed: {resp.message}")
        
    query_embedding = resp.output['embeddings'][0]['embedding']
    # 3. æ•°æ®åº“æ£€ç´¢
    conn = get_db_connection()
    cur = conn.cursor()
    
    sql = f"""
        SELECT game_id, category, content, 1 - (embedding <=> %s::vector) as similarity
        FROM documents
        WHERE 1=1
    """
    params = [query_embedding]
    
    if intent != 'all':
        sql += " AND category = %s"
        params.append(intent)

    if game_id:
        sql += " AND (game_id = %s OR game_id IS NULL)"
        params.append(game_id)
        
    sql += f" ORDER BY embedding <=> %s::vector LIMIT {Config.TOP_K}"
    params.append(query_embedding)
    
    print(f"====== [PGSQL DEBUG] Executing: {sql.replace(chr(10), ' ').strip()} ======")
    cur.execute(sql, tuple(params))
    results = cur.fetchall()
    cur.close()
    conn.close()
    
    return intent, results

@app.get("/rag/search")
async def search_document(query: str, game_id: int = None):
    """è¿”å›æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ"""
    if not query:
        raise HTTPException(status_code=400, detail="Query is required")
    try:
        intent, results = _retrieve_documents(query, game_id)
        return {
            "intent": intent,
            "results": [
                {
                    "game_id": row[0],
                    "category": row[1],
                    "content": row[2], 
                    "similarity": float(row[3])
                } for row in results
            ]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

class AskRequest(BaseModel):
    query: str
    game_id: Optional[int] = None

@app.post("/rag/ask")
async def ask_question(request: AskRequest):
    """RAG é—®ç­”ï¼šæ£€ç´¢ + ç”Ÿæˆå›ç­”"""
    if not request.query:
        raise HTTPException(status_code=400, detail="Query is required")
        
    try:
        # Step 1: æ£€ç´¢
        intent, results = _retrieve_documents(request.query, request.game_id)
        
        if not results:
            return {"answer": "æŠ±æ­‰ï¼Œæˆ‘çš„çŸ¥è¯†åº“é‡Œæš‚æ—¶æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œæ‚¨å¯ä»¥æ¢ä¸ªè¯´æ³•è¯•è¯•ã€‚", "sources": []}
            
        # Step 2: æ•´ç†ä¸Šä¸‹æ–‡
        # æå– content å­—æ®µï¼Œæ‹¼æ¥èµ·æ¥
        context_list = [f"ã€èµ„æ–™{i+1}ã€‘: {row[2]}" for i, row in enumerate(results)]
        
        # --- æ–°å¢: è·å–ç›¸å…³æ¸¸æˆçš„åº“å­˜ä¿¡æ¯ ---
        # æ”¶é›†æ¶‰åŠçš„ game_id (æ’é™¤ None)
        related_game_ids = set()
        if request.game_id:
            related_game_ids.add(request.game_id)
        
        for row in results:
            if row[0]: # row[0] is game_id
                related_game_ids.add(row[0])
                
        if related_game_ids:
            try:
                stock_map = get_games_stock(list(related_game_ids))
                stock_info_strs = []
                for gid, info in stock_map.items():
                    stock_info_strs.append(f"æ¸¸æˆã€Š{info['title']}ã€‹å½“å‰å‰©ä½™åº“å­˜ï¼š{info['available_stock']}ä»½")
                
                if stock_info_strs:
                    context_list.append("\nã€å®æ—¶åº“å­˜ä¿¡æ¯ã€‘:\n" + "\n".join(stock_info_strs))
            except Exception as e:
                print(f"Failed to fetch stock info: {e}")
        # ----------------------------------

        context_str = "\n\n".join(context_list)
        
        # Step 3: è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›ç­” (Generation)
        prompt = Config.RAG_ANSWER_PROMPT.format(context=context_str, query=request.query)
        
        print(f"\n====== [RAG FINAL PROMPT] ======\n{prompt}\n==============================\n")
        
        # Retry logic for LLM call
        import time
        max_retries = 3
        gen_resp = None
        
        for attempt in range(max_retries):
            try:
                gen_resp = Generation.call(
                    model=Generation.Models.qwen_turbo,
                    prompt=prompt
                )
                if gen_resp.status_code == 200:
                    break
                else:
                    print(f"âš ï¸ LLM Call failed (Status {gen_resp.status_code}, Attempt {attempt+1}/{max_retries}): {gen_resp.message}")
            except Exception as e:
                print(f"âš ï¸ LLM Network/SSL Error (Attempt {attempt+1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    raise HTTPException(status_code=500, detail=f"LLM Service Unavailable: {str(e)}")
                time.sleep(1) # wait 1s before retry
        
        if gen_resp.status_code == 200:
            answer = gen_resp.output.text
            return {
                "answer": answer,
                "intent": intent,
                "sources": [
                    {"content": row[2], "similarity": float(row[3])} 
                    for row in results
                ]
            }
        else:
            raise HTTPException(status_code=500, detail=f"Generation failed: {gen_resp.message}")

    except Exception as e:
        print(f"Error in ask: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == '__main__':
    print("Starting RAG Service with FastAPI...")
    uvicorn.run(app, host="0.0.0.0", port=5001)
