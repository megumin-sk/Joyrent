# 🔍 JoyRent RAG 搜索与问答引擎 (RAG Search & QA Engine)

> **💡 这不仅是一个 AI 项目，更是一个完整的“AI + 业务”落地案例。**
> 如果说 `RAG_Intend_Bert` 是闭门修炼的“大脑”，那么本模块（`RAG_search`）就是走上岗位的“超级客服”。

---

## 🌟 1. 它是做什么的？（业务视角）

这是 JoyRent 平台的“聪明大脑”。它能做三件事：
1.  **听懂人话**：加载微调好的 BERT 模型，秒懂用户是在问规则还是找游戏（意图识别）。
2.  **翻阅文档**：在千万级别的知识库里，零点几秒内精准找到对应的规则或攻略（向量检索）。
3.  **开口解答**：把找到的资料整理好，配合实时库存数据，生成一段有礼貌、有温度的回答（LLM 生成）。

---

## 📂 2. 核心架构与目录

```text
RAG_search/
├── src/
│   ├── rag_service.py      # 🚀 【中心调度枢纽】：基于 FastAPI，控制整个 AI 的工作流。
│   ├── config.py           # ⚙️ 【全局配置】：管理数据库地址、API 秘钥及 BERT 模型路径。
│   ├── database.py         # 🗄️ 【向量数据库】：连接 PostgreSQL，负责存储和检索语义特征向量（Embedding）。
│   ├── document_loader.py  # ✂️ 【文档切分】：把长篇大论的规则切成“易消化”的小段。
│   └── model_factory.py    # 🏭 【通用工厂】：与训练模块保持一致，负责加载训练好的 BERT “记忆”。
├── platform_rules.md       # 📚 【知识库源码】：平台的原始规则文档。
└── requirements.txt        # 📦 【依赖清单】：记录了所有的运行环境要求。
```

---

## 🚀 3. 三大核心技术亮点（复试必杀技）

### ⚡ A. 意图驱动的精准检索 (Intent-Driven Retrieval)
*   **面试标准回答**：
    “我在检索流程中加入了一个**意图过滤器**。系统收到用户提问时，会先通过 BERT 进行分类。如果识别为‘规则类’，搜索逻辑会自动过滤数据库中的‘游戏攻略’内容。**这有效解决了相关性偏差问题**，比如当用户说‘归还’时，系统只会搜出租赁协议，而不会被某款游戏剧情里的‘归还道具’字眼所干扰。”

### ⚡ B. 混合数据增强 (Hybrid Context Enrichment)
*   **面试标准回答**：
    “我们的 RAG 不仅仅是检索静态文档。在生成回答前，系统会**实时调用数据库接口获取游戏库存**。最终投喂给大模型的上下文包含：‘静态规则’+‘游戏文档’+‘动态库存’。这比单纯的静态知识库更具业务价值，确保了 AI 不会告诉用户一款已经断货的游戏‘现在可以租’。”

### ⚡ C. 向量数据库选型与优化
*   **面试标准回答**：
    “项目中使用了 **PostgreSQL + pgvector** 扩展来存储和检索 1536 维的向量数据。我使用了 `cosine_similarity`（余弦相似度）进行比对，并在 SQL 层面实现了意图分类与向量相似度的‘混合过滤’，检索性能在毫秒级别。”

---

## 🛠️ 4. AI 运行的全过程（复试对着它画图）

1.  **输入**：用户问：“我想租 Switch，押金多少？还有黑神话吗？”
2.  **分类**：`RAG_Intend_Bert` 模型告诉我们，这是个“混合意图（all）”。
3.  **检索**：
    -   去“规则库”搜：搜出押金退还规则。
    -   去“游戏库”搜：搜出黑神话悟空的游戏详情。
4.  **动态查询**：后台自动查询数据库，发现《黑神话》当前库存剩余 3 份。
5.  **生成**：大模型（Qwen）把所有信息揉在一起，生成一段专业回答。

---

## 🏁 5. 复试疑难点快速反击

*   **Q：为什么不用现成的 LangChain 或者向量库软件？**
    *   *A：为了更深度的业务定制。自己基于 FastAPI 和 pgvector 实现，可以更灵活地处理‘意图分发’逻辑，并在检索层直接与业务数据库打通，避免了多套系统的同步延迟。*
*   **Q：大模型胡说八道（幻觉）怎么办？**
    *   *A：我在 Prompt 里加入了‘严格约束’：‘如果参考资料中没有，请诚实说不知道，不要编造’。同时设置了 0.6 的检索阈值，低于这个分数的资料不会送给大模型，从源头切断错误信息。*
*   **Q：向量化模型用的是哪个？**
    *   *A：使用的是阿里云 DashScope 的 `text_embedding_v1`，它支持 1536 维向量，对中文语境理解非常出色。*
