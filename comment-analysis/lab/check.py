import json
import os

# ================= é…ç½®åŒºåŸŸ =================
# è¿™é‡Œå¡«ä½ æ‰€æœ‰éœ€è¦æ£€æŸ¥çš„æ–‡ä»¶è·¯å¾„
# å»ºè®®æ£€æŸ¥å¤„ç†åçš„ä¸‰ä¸ªæ–‡ä»¶ï¼Œçœ‹çœ‹æœ‰æ²¡æœ‰é‡å 
FILE_PATHS = {
    'train': r'D:\workspace\JoyRent\SwitchRent\comment-analysis\data\processed\train.json',
    'val':   r'D:\workspace\JoyRent\SwitchRent\comment-analysis\data\processed\val.json',
    'test':  r'D:\workspace\JoyRent\SwitchRent\comment-analysis\data\processed\test.json'
}
# ===========================================

def load_texts(file_path):
    """è¯»å–æ–‡ä»¶å¹¶æå–æ‰€æœ‰æ–‡æœ¬"""
    if not os.path.exists(file_path):
        print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {file_path}")
        return []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å« (ç´¢å¼•, æ–‡æœ¬å†…å®¹)
    return [(i, item.get('text', '').strip()) for i, item in enumerate(data)]

def check_internal_duplicates(name, texts_with_id):
    """æ£€æŸ¥å•ä¸ªæ–‡ä»¶å†…éƒ¨çš„é‡å¤"""
    print(f"\nğŸ” æ­£åœ¨æ£€æŸ¥æ–‡ä»¶å†…éƒ¨é‡å¤: ã€{name}ã€‘")
    
    seen = {} # map: text -> index
    duplicates = []
    
    for idx, text in texts_with_id:
        if text in seen:
            duplicates.append((seen[text], idx, text))
        else:
            seen[text] = idx
            
    if not duplicates:
        print("   âœ… å¹²å‡€ï¼æ— å†…éƒ¨é‡å¤ã€‚")
    else:
        print(f"   ğŸš¨ å‘ç° {len(duplicates)} ç»„é‡å¤æ•°æ®ï¼")
        for orig_idx, curr_idx, text in duplicates[:3]: # åªæ‰“å°å‰3ä¸ªä¾‹å­
            print(f"      - ç´¢å¼• {curr_idx} ä¸ç´¢å¼• {orig_idx} é‡å¤: {text[:30]}...")

    return set([t for _, t in texts_with_id]) # è¿”å›çº¯æ–‡æœ¬é›†åˆç”¨äºè·¨æ–‡ä»¶æ¯”è¾ƒ

def check_data_leakage(sets_dict):
    """æ£€æŸ¥è·¨æ–‡ä»¶çš„æ•°æ®æ³„éœ² (Data Leakage)"""
    print(f"\nğŸ•µï¸ æ­£åœ¨æ£€æŸ¥æ•°æ®æ³„éœ² (è·¨æ–‡ä»¶é‡å¤)...")
    
    # æ£€æŸ¥ Train vs Test (æœ€ä¸¥é‡çš„æ³„éœ²)
    if 'train' in sets_dict and 'test' in sets_dict:
        intersection = sets_dict['train'].intersection(sets_dict['test'])
        if intersection:
            print(f"   ğŸš¨ ä¸¥é‡è­¦å‘Šï¼Train å’Œ Test ä¹‹é—´æœ‰ {len(intersection)} æ¡é‡å¤æ•°æ®ï¼(æ¨¡å‹åœ¨ä½œå¼Š)")
            print(f"      ç¤ºä¾‹: {list(intersection)[0][:30]}...")
        else:
            print("   âœ… Train ä¸ Test æ— äº¤é›† (å®‰å…¨)ã€‚")
            
    # æ£€æŸ¥ Train vs Val
    if 'train' in sets_dict and 'val' in sets_dict:
        intersection = sets_dict['train'].intersection(sets_dict['val'])
        if intersection:
            print(f"   âš ï¸ è­¦å‘Šï¼šTrain å’Œ Val ä¹‹é—´æœ‰ {len(intersection)} æ¡é‡å¤æ•°æ®ã€‚")
        else:
            print("   âœ… Train ä¸ Val æ— äº¤é›†ã€‚")

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æ•°æ®é‡å¤æ€§æ£€æŸ¥...")
    
    # 1. åŠ è½½æ‰€æœ‰æ•°æ®
    text_sets = {}
    for name, path in FILE_PATHS.items():
        texts = load_texts(path)
        if texts:
            # 2. æ£€æŸ¥æ¯ä¸ªæ–‡ä»¶å†…éƒ¨æ˜¯å¦æœ‰é‡å¤
            unique_texts = check_internal_duplicates(name, texts)
            text_sets[name] = unique_texts
            
    # 3. æ£€æŸ¥æ–‡ä»¶ä¹‹é—´æ˜¯å¦æœ‰é‡å¤ (æ•°æ®æ³„éœ²)
    check_data_leakage(text_sets)
    
    print("\nå®Œæˆã€‚å¦‚æœå‘ç°ä¸¥é‡æ³„éœ²ï¼Œå»ºè®®é‡æ–°è¿è¡Œ split_data.py ä¹‹å‰å…ˆå¯¹åŸå§‹ dataset.json è¿›è¡Œå»é‡ã€‚")