import os
import sys
import numpy as np
import torch
from datasets import load_dataset
from transformers import Trainer, TrainingArguments, DataCollatorWithPadding
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# ------------------------------------------------------------------
# ğŸ”— è·¯å¾„é…ç½®ï¼šå°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥ Python æœç´¢è·¯å¾„
# ------------------------------------------------------------------
# å‡è®¾ train.py ä½äºé¡¹ç›®æ ¹ç›®å½•
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.append(project_root)

# å¯¼å…¥é…ç½®å’Œå·¥å‚ç±»
from config.train_config import TrainConfig
from src.model_factory import ModelFactory 
# ------------------------------------------------------------------

def compute_metrics(eval_pred):
    """
    è‡ªå®šä¹‰è¯„ä¼°å‡½æ•°ï¼šè®¡ç®—å‡†ç¡®ç‡å’Œ F1 åˆ†æ•°
    """
    predictions, labels = eval_pred
    # predictions æ˜¯ logitsï¼Œå–æœ€å¤§å€¼çš„ç´¢å¼•ä½œä¸ºé¢„æµ‹ç»“æœ
    preds = np.argmax(predictions, axis=1)
    
    # average='weighted' ç”¨äºå¤šåˆ†ç±»ä»»åŠ¡ï¼Œè€ƒè™‘ç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µ
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, preds, average='weighted', zero_division=0
    )
    acc = accuracy_score(labels, preds)
    
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

def main():
    # 1. æ‰“å°é…ç½®ä¿¡æ¯
    TrainConfig.show_info()
    
    # 2. æ£€æŸ¥æ–‡ä»¶è·¯å¾„
    if not os.path.exists(TrainConfig.TRAIN_FILE) or not os.path.exists(TrainConfig.DEV_FILE):
        print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è®­ç»ƒé›†æˆ–éªŒè¯é›† CSV æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®åˆ’åˆ†è„šæœ¬ã€‚")
        return

    # ============================================================
    # ğŸ—ï¸ é˜¶æ®µä¸€ï¼šä½¿ç”¨å·¥å‚åŠ è½½ç»„ä»¶ (è§£è€¦çš„æ ¸å¿ƒ)
    # ============================================================
    print("\nğŸ­ æ­£åœ¨ä»å·¥å‚åŠ è½½æ¨¡å‹ç»„ä»¶...")
    
    # A. è·å–åˆ†è¯å™¨
    tokenizer = ModelFactory.get_tokenizer(TrainConfig.PRETRAINED_MODEL_NAME)
    
    # B. è·å–æ¨¡å‹
    model = ModelFactory.get_model(
        model_path=TrainConfig.PRETRAINED_MODEL_NAME,
        num_labels=TrainConfig.NUM_LABELS,
        id2label=TrainConfig.ID2LABEL,
        label2id=TrainConfig.LABEL2ID
    )

    # ============================================================
    # ğŸ“‚ é˜¶æ®µäºŒï¼šæ•°æ®åŠ è½½ä¸é¢„å¤„ç† (ç›´æ¥å†™åœ¨è®­ç»ƒæµç¨‹ä¸­)
    # ============================================================
    print("\nğŸ“‚ æ­£åœ¨åŠ è½½å¹¶å¤„ç†æ•°æ®...")
    data_files = {"train": TrainConfig.TRAIN_FILE, "validation": TrainConfig.DEV_FILE}
    
    # åŠ è½½ CSV æ•°æ®é›†
    dataset = load_dataset("csv", data_files=data_files)

    def preprocess_function(examples):
        """å°†æ–‡æœ¬è½¬æ¢ä¸º Token ID å’Œ Attention Mask"""
        tokenized_inputs = tokenizer(
            examples["text"], 
            truncation=True, 
            padding=False, # ä½¿ç”¨åŠ¨æ€å¡«å……ï¼Œæ›´é«˜æ•ˆ
            max_length=TrainConfig.MAX_LEN
        )
        
        # å°†æ–‡æœ¬æ ‡ç­¾è½¬æ¢ä¸ºæ•°å­— ID
        tokenized_inputs["labels"] = [TrainConfig.LABEL2ID[label] for label in examples["label"]]
        return tokenized_inputs

    # åº”ç”¨é¢„å¤„ç†ï¼Œå¹¶ç§»é™¤åŸå§‹çš„ text å’Œ label åˆ—
    tokenized_datasets = dataset.map(
        preprocess_function, 
        batched=True, 
        remove_columns=["text", "label"]
    )
    
    # ============================================================
    # ğŸš€ é˜¶æ®µä¸‰ï¼šé…ç½® Trainer å¹¶å¼€å§‹è®­ç»ƒ
    # ============================================================
    print("\nâš™ï¸ æ­£åœ¨é…ç½®è®­ç»ƒå‚æ•°...")
    training_args = TrainingArguments(
        output_dir=TrainConfig.OUTPUT_DIR,
        num_train_epochs=TrainConfig.EPOCHS,
        per_device_train_batch_size=TrainConfig.BATCH_SIZE,
        per_device_eval_batch_size=TrainConfig.BATCH_SIZE,
        learning_rate=TrainConfig.LEARNING_RATE,
        weight_decay=TrainConfig.WEIGHT_DECAY,
        warmup_ratio=TrainConfig.WARMUP_RATIO,
        
        eval_strategy="epoch", 
        save_strategy="epoch",       
        logging_dir=TrainConfig.LOG_DIR,
        logging_steps=10,           
        load_best_model_at_end=True, 
        metric_for_best_model="f1", # ä½¿ç”¨ F1 Score ä½œä¸ºæœ€ä½³æ¨¡å‹æ ‡å‡†ï¼Œå› ä¸ºå®ƒæ¯” Accuracy å¯¹ç±»åˆ«ä¸å¹³è¡¡æ›´æ•æ„Ÿ
        save_total_limit=1,         # åªä¿ç•™æœ€ä½³æ¨¡å‹ï¼ŒèŠ‚çœç©ºé—´
        
        dataloader_num_workers=TrainConfig.NUM_WORKERS,
        seed=TrainConfig.SEED,
    )

    # åˆå§‹åŒ– Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        tokenizer=tokenizer,
        # DataCollatorWithPadding å¯ä»¥åœ¨æ‰¹æ¬¡å†…éƒ¨å¡«å……åˆ°æœ€é•¿å¥å­çš„é•¿åº¦ï¼Œè€Œä¸æ˜¯MAX_LENï¼ŒèŠ‚çœè®¡ç®—
        data_collator=DataCollatorWithPadding(tokenizer=tokenizer), 
        compute_metrics=compute_metrics,
    )

    # å¼€å§‹è®­ç»ƒ 
    print("\nğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    trainer.train()

    # ============================================================
    # ğŸ’¾ é˜¶æ®µå››ï¼šä¿å­˜æœ€ç»ˆäº§å‡ºç‰©
    # ============================================================
    print("\nğŸ“Š è®­ç»ƒå·²å®Œæˆï¼Œæ­£åœ¨è¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
    trainer.evaluate()

    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {TrainConfig.OUTPUT_DIR}")
    trainer.save_model(TrainConfig.OUTPUT_DIR)
    tokenizer.save_pretrained(TrainConfig.OUTPUT_DIR) 
    
    print("\nâœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åœ¨ output ç›®å½•ã€‚")

if __name__ == "__main__":
    main()